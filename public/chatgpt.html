<!DOCTYPE html>
<html lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
<style>
  @keyframes slideUpFade {
    from { transform: translateY(20px); opacity: 0; }
    to { transform: translateY(0); opacity: 1; }
  }
  body {
    background-color: #ff9999;
    display: flex;
    align-items: center;
    flex-direction: column;
  }
  #camera {
    border: none;
    border-radius: 10px;
    box-shadow: 0 0 10px rgba(0,0,0,0.5);
    display: block;
    width: 300px;
    height: 400px;
    object-fit: cover;
    padding: 5px;
    animation: slideUpFade 0.5s ease-out;
  }
  #previewcanvas {
    padding: 5px;
    border: none;
    border-radius: 10px;
    max-height: 90vh;
    max-width: 90vw;
    height: auto;
    width: auto;
    z-index: 1002;
    border-radius: 10px;
    border: none;
  }
  #btn2 {
    background-color: FloralWhite;
    border-radius: 10px;
    border: none;
    padding: 10px;
    animation: slideUpFade 0.5s ease-out;
  }
  #btn2:hover {
    background-color: red;
    transform: scale(1.03);
    transition: all 0.2s ease-in-out;
  }
  #overlay, #newoverlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100vw;
    height: 100vh;
    visibility: hidden;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    z-index: 1001;
    backdrop-filter: blur(8px);
    background-color: rgba(0,0,0,0.5);
    opacity: 0;
    transition: opacity 0.3s ease;
  }
  #newoverlay div {
    background-color: white;
    padding: 20px 30px;
    border-radius: 10px;
    font-size: 1.5em;
    text-align: center;
    color: black;
    box-shadow: 0 0 15px rgba(0,0,0,0.5);
  }
  #overlay.show, #newoverlay.show {
    visibility: visible;
    opacity: 1;
  }
</style>
<title>Face Comparison</title>
</head>
<body>
<div id="main" style="display:none;">
  <div id="newoverlay">
    <div id="message">
      <p id="p">Capturing second face...</p>
    </div>
  </div>
  <div id="overlay">
    <canvas id="previewcanvas"></canvas>
    <button id="btn2" style="display:none;">Compare Faces</button>
  </div>
  <div style="position:relative; width:300px; height:400px;">
    <video id="camera" width="300" height="400" autoplay></video>
    <canvas id="overlaycanvas" style="position:absolute; top:0; left:0; width:300px; height:400px;"></canvas>
  </div>
</div>

<script>
  const video = document.getElementById('camera');
  const previewcanvas = document.getElementById('previewcanvas');
  const previewCtx = previewcanvas.getContext('2d');
  const main = document.getElementById('main');
  const savebtn = document.getElementById('btn2');
  const overlaycanvas = document.getElementById("overlaycanvas");
  const overlayCtx = overlaycanvas.getContext('2d');
  const overlay = document.getElementById("overlay");
  const newoverlay = document.getElementById('newoverlay');

  let detectionPaused = false;

  async function loadModels() {
    await faceapi.nets.tinyFaceDetector.loadFromUri('/models/');
    await faceapi.nets.faceLandmark68Net.loadFromUri('/models/');
    await faceapi.nets.faceRecognitionNet.loadFromUri('/models/');
    console.log("âœ¨ All face-api models are fully loaded");
  }

  function showCapturedFace(start, area) {
    newoverlay.classList.add('show');
    void newoverlay.offsetWidth;
    setTimeout(() => {
      newoverlay.classList.remove('show');
      overlay.classList.add('show');
      previewCtx.clearRect(0, 0, previewcanvas.width, previewcanvas.height);
      previewCtx.drawImage(
        video,
        start[0], start[1], area[0], area[1],
        0, 0, previewcanvas.width, previewcanvas.height
      );
      savebtn.style.display = "block";
    }, 3000);
  }

  function startCameraAndDetect() {
    navigator.mediaDevices.getUserMedia({ video: true })
      .then(stream => {
        video.srcObject = stream;
        main.style.display = "block";

        video.addEventListener("loadeddata", () => {
          overlaycanvas.width = video.videoWidth;
          overlaycanvas.height = video.videoHeight;

          const interval = setInterval(async () => {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors();
            
            if (detections.length > 0 && !detectionPaused) {
              detectionPaused = true;

              const start = detections[0].detection.box.topLeft;
              const area = [detections[0].detection.box.width, detections[0].detection.box.height];
              showCapturedFace(start, area);

              window.capturedDescriptor = detections[0].descriptor;
            }
          }, 200);
        });
      })
      .catch(err => {
        console.error('Camera error:', err);
        alert('Please allow camera access!');
      });
  }

  loadModels().then(() => {
    console.log("ðŸŒ™ Models loaded, starting camera now.");
    startCameraAndDetect();
  });

  savebtn.addEventListener("click", async () => {
    const oldFaceData = sessionStorage.getItem('imageData');
    if (!oldFaceData) {
      alert("No reference face found in session storage.");
      return;
    }

    const img = new Image();
    img.src = oldFaceData;
    img.onload = async () => {
      const refDetection = await faceapi.detectSingleFace(img, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptor();

      if (!refDetection) {
        alert("Reference face not detected in stored image.");
        return;
      }

      const refDescriptor = refDetection.descriptor;
      const newDescriptor = window.capturedDescriptor;

      const distance = faceapi.euclideanDistance(refDescriptor, newDescriptor);
      const threshold = 0.6;

      if (distance < threshold) {
        alert("ðŸ˜Š Faces matched! It's the same person.");
      } else {
        alert("ðŸ˜¢ Faces did not match.");
      }
    };
  });
</script>
</body>
</html>
